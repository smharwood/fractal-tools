{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/smharwood/chaos-game/blob/master/GAN_TF2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rF2x3qooyBTI"
      },
      "source": [
        "# Deep Convolutional Generative Adversarial Network\n",
        "\n",
        "This tutorial (from https://www.tensorflow.org/tutorials/generative/dcgan) demonstrates how to generate images using a [Deep Convolutional Generative Adversarial Network](https://arxiv.org/pdf/1511.06434.pdf) (DCGAN). The code is written using the [Keras Sequential API](https://www.tensorflow.org/guide/keras) with a `tf.GradientTape` training loop."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSL3UvXi2YeX"
      },
      "source": [
        "TODO:\n",
        "\n",
        "- Play with batch size? May need to be smaller for larger image size\n",
        "- Generator/discriminator capacity?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1_Y75QXJS6h"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "YfIk2es3hJEd",
        "outputId": "c0fa1659-1cd4-4944-c74c-3088420c2d9d"
      },
      "outputs": [],
      "source": [
        "import os, glob, time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import imageio, PIL\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "tf.__version__"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iYn4MdZnKCey"
      },
      "source": [
        "### Load and prepare the dataset\n",
        "\n",
        "Upload custom dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4PIDhoDLbsZ"
      },
      "outputs": [],
      "source": [
        "BUFFER_SIZE = 6000\n",
        "BATCH_SIZE = 64\n",
        "images_dir = \"images\"\n",
        "\n",
        "image_size = 32"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NSIB9gpEe02",
        "outputId": "64b1b95f-469a-4cf0-cede-93a0abd52868"
      },
      "outputs": [],
      "source": [
        "from google.colab import files, drive\n",
        "\n",
        "# mount Drive\n",
        "mount_point = \"/content/drive\"\n",
        "drive.mount(mount_point, force_remount=False)\n",
        "\n",
        "# Get dataset as zipfile - either upload or find in Drive \n",
        "if False:\n",
        "  uploaded = files.upload()\n",
        "  zip_name = list(uploaded.keys())[0]\n",
        "  assert zip_name.split('.')[1] == \"zip\", \"Expecting a zipfile\"\n",
        "else:\n",
        "  zip_name = mount_point + \"/MyDrive/chaosGAN/images_{}.zip\".format(image_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xk_g_o_S2jy_"
      },
      "outputs": [],
      "source": [
        "import zipfile\n",
        "\n",
        "# Extract files from zip -- expecting a \"flat\" file structure (no subdirectory)\n",
        "with zipfile.ZipFile(zip_name, 'r') as zipObj:\n",
        "  zipObj.extractall(images_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E3XJ0xS7E0Be",
        "outputId": "9c096d07-656a-4960-ba9d-ed5e4b63eb2a"
      },
      "outputs": [],
      "source": [
        "image_files = glob.glob(images_dir+\"/*.png\")\n",
        "ds_length = len(image_files)\n",
        "print(\"Data directory: {}\".format(images_dir))\n",
        "print(\"First 5 image files in data dir: {}\".format(image_files[:5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixzs5XkqFrgk"
      },
      "outputs": [],
      "source": [
        "#img_height = image_size\n",
        "#img_width = image_size\n",
        "def _preprocess(img_name):\n",
        "  # take filename string, read file,\n",
        "  # decode bytes to appropriate image format as a 3D(?) uint8 tensor,\n",
        "  # resize, \n",
        "  # and map to [-1,1]\n",
        "  img = tf.io.read_file(img_name)\n",
        "  img = tf.io.decode_png(img, channels=1)\n",
        "  #img = tf.image.resize(img, [img_height, img_width])\n",
        "  return (tf.cast(img, tf.float32) - 127.5) / 127.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a4fYMGxGhrna"
      },
      "outputs": [],
      "source": [
        "# Map, batch and shuffle the data\n",
        "train_dataset = (tf.data.Dataset.from_tensor_slices(image_files)\n",
        "                  .map(_preprocess)\n",
        "                  .cache()\n",
        "                  .shuffle(BUFFER_SIZE)\n",
        "                  .batch(BATCH_SIZE)\n",
        "                  .prefetch(tf.data.experimental.AUTOTUNE)\n",
        "                )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgLuqFD6_D8w"
      },
      "outputs": [],
      "source": [
        "# MNIST FOR TESTING\n",
        "#BUFFER_SIZE = 60000\n",
        "#BATCH_SIZE = 256\n",
        "#(train_images, train_labels), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "#train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype('float32')\n",
        "#train_images = (train_images - 127.5) / 127.5  # Normalize the images to [-1, 1]\n",
        "#train_dataset = tf.data.Dataset.from_tensor_slices(train_images).shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "THY-sZMiQ4UV"
      },
      "source": [
        "## Create the models\n",
        "\n",
        "Both the generator and discriminator are defined using the [Keras Sequential API](https://www.tensorflow.org/guide/keras#sequential_model)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-tEyxE-GMC48"
      },
      "source": [
        "### The Generator\n",
        "\n",
        "The generator uses `tf.keras.layers.Conv2DTranspose` (upsampling) layers to produce an image from a seed (random noise). Start with a `Dense` layer that takes this seed as input, then upsample several times until you reach the desired image size of 28x28x1. Notice the `tf.keras.layers.LeakyReLU` activation for each layer, except the output layer which uses tanh (which ensures outputs are in [-1,1])."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9x8yINkT3FZ"
      },
      "source": [
        "The chaos game that generates the dataset is controlled by about 20 uniformly random parameters.\n",
        "No real need for a huge hidden dimension."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybSxmK23PpQh"
      },
      "outputs": [],
      "source": [
        "noise_dim = 16"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lOeL-NaYvX8"
      },
      "outputs": [],
      "source": [
        "def get_DenseDefault(output_size, l2_weight):\n",
        "  return layers.Dense(output_size,\n",
        "                      kernel_regularizer=tf.keras.regularizers.L2(l2_weight),\n",
        "                      bias_regularizer=tf.keras.regularizers.L2(l2_weight))\n",
        "  \n",
        "def get_Conv2DTransposeDefault(filters, kernel_size, strides, l2_weight, activation=None):\n",
        "  return layers.Conv2DTranspose(filters, kernel_size, strides=strides, padding='same',\n",
        "                       activation=activation,\n",
        "                       kernel_regularizer=tf.keras.regularizers.L2(l2_weight),\n",
        "                       bias_regularizer=tf.keras.regularizers.L2(l2_weight))\n",
        "  \n",
        "def get_Conv2DDefault(filters, kernel_size, strides, l2_weight, activation=None):\n",
        "  return layers.Conv2D(filters, kernel_size, strides=strides, padding='same',\n",
        "                       activation=activation,\n",
        "                       kernel_regularizer=tf.keras.regularizers.L2(l2_weight),\n",
        "                       bias_regularizer=tf.keras.regularizers.L2(l2_weight))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TKLcnvRUYKzS"
      },
      "outputs": [],
      "source": [
        "def make_generator_model_32(l2_weight=2.5e-5):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(1024, input_shape=(noise_dim,),\n",
        "                           use_bias=False,\n",
        "                           kernel_regularizer=tf.keras.regularizers.L2(l2_weight)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(get_DenseDefault(8*8*16, l2_weight))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((8, 8, 16)))\n",
        "    assert model.output_shape == (None, 8, 8, 16)  # Note: None is the batch size\n",
        "\n",
        "    model.add(get_Conv2DTransposeDefault(16, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    assert model.output_shape == (None, 16, 16, 16)\n",
        "    #model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(get_Conv2DTransposeDefault(16, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    assert model.output_shape == (None, 32, 32, 16)\n",
        "    #model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(get_Conv2DDefault(1, (4, 4), strides=(1, 1), l2_weight=0.0, activation='tanh'))\n",
        "    assert model.output_shape == (None, 32, 32, 1)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nKGLfns3yfhh"
      },
      "outputs": [],
      "source": [
        "def make_generator_model_128(l2_weight=2.5e-5):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Dense(1024, input_shape=(noise_dim,),\n",
        "                           use_bias=False,\n",
        "                           kernel_regularizer=tf.keras.regularizers.L2(l2_weight)))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(get_DenseDefault(8*8*16, l2_weight))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(layers.Reshape((8, 8, 16)))\n",
        "    assert model.output_shape == (None, 8, 8, 16)  # Note: None is the batch size\n",
        "\n",
        "    model.add(get_Conv2DTransposeDefault(16, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    assert model.output_shape == (None, 16, 16, 16)\n",
        "    #model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(get_Conv2DTransposeDefault(16, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    assert model.output_shape == (None, 32, 32, 16)\n",
        "    #model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(get_Conv2DTransposeDefault(16, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    assert model.output_shape == (None, 64, 64, 16)\n",
        "    #model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(get_Conv2DTransposeDefault(16, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    assert model.output_shape == (None, 128, 128, 16)\n",
        "    #model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "\n",
        "    model.add(get_Conv2DDefault(1, (4, 4), strides=(1, 1), l2_weight=0.0, activation='tanh'))\n",
        "    assert model.output_shape == (None, 128, 128, 1)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GyWgG09LCSJl"
      },
      "source": [
        "Use the (as yet untrained) generator to create an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 284
        },
        "id": "gl7jcC7TdPTG",
        "outputId": "3fe3c4d2-28db-4703-af94-0a97b7dd1b0b"
      },
      "outputs": [],
      "source": [
        "generator_dict = {32:  make_generator_model_32,\n",
        "                  128: make_generator_model_128}\n",
        "\n",
        "generator = generator_dict[image_size]()\n",
        "\n",
        "noise = tf.random.normal([1, noise_dim])\n",
        "generated_image = generator(noise, training=False)\n",
        "\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D0IKnaCtg6WE"
      },
      "source": [
        "### The Discriminator\n",
        "\n",
        "The discriminator is a CNN-based image classifier.\n",
        "Note: no final activation; output is in (-inf, inf)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R0ahm8JLYwMx"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model_32(l2_weight=2.5e-5):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same',\n",
        "                            input_shape=[32, 32, 1],\n",
        "                            kernel_regularizer=tf.keras.regularizers.L2(l2_weight),\n",
        "                            bias_regularizer=tf.keras.regularizers.L2(l2_weight)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(get_Conv2DDefault(128, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    #model.add(get_Conv2DDefault(64, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    #model.add(layers.LeakyReLU())\n",
        "    #model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(get_DenseDefault(1024, l2_weight))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(get_DenseDefault(1, l2_weight))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dBuaX9mNzxDl"
      },
      "outputs": [],
      "source": [
        "def make_discriminator_model_128(l2_weight=2.5e-5):\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(layers.Conv2D(128, (4, 4), strides=(2, 2), padding='same',\n",
        "                            input_shape=[128, 128, 1],\n",
        "                            kernel_regularizer=tf.keras.regularizers.L2(l2_weight),\n",
        "                            bias_regularizer=tf.keras.regularizers.L2(l2_weight)))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(get_Conv2DDefault(128, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(layers.Dropout(0.3))\n",
        "\n",
        "    #model.add(get_Conv2DDefault(64, (4, 4), strides=(2, 2), l2_weight=l2_weight))\n",
        "    #model.add(layers.LeakyReLU())\n",
        "    #model.add(layers.Dropout(0.3))\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "    model.add(get_DenseDefault(1024, l2_weight))\n",
        "    model.add(layers.BatchNormalization())\n",
        "    model.add(layers.LeakyReLU())\n",
        "    model.add(get_DenseDefault(1, l2_weight))\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhPneagzCaQv"
      },
      "source": [
        "Use the (as yet untrained) discriminator to classify the generated images as real or fake."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDkA05NE6QMs",
        "outputId": "95354239-4c56-4ca7-a2fa-2bbb692dfdb7"
      },
      "outputs": [],
      "source": [
        "discriminator_dict = {32:  make_discriminator_model_32,\n",
        "                      128: make_discriminator_model_128}\n",
        "\n",
        "discriminator = discriminator_dict[image_size]()\n",
        "\n",
        "decision = discriminator(generated_image)\n",
        "print (decision)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0FMYgY_mPfTi"
      },
      "source": [
        "## Define the loss and optimizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psQfmXxYKU3X"
      },
      "outputs": [],
      "source": [
        "# This method returns a helper function to compute cross entropy loss\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKY_iPSPNWoj"
      },
      "source": [
        "### Discriminator loss\n",
        "\n",
        "This method quantifies how well the discriminator is able to distinguish real images from fakes. Generally, we want the discriminator to output positive values for real images, and negative values for fake images.\n",
        "\n",
        "We could interpret these real values as logits, and do a cross-entropy thing, but based on the [\"Wasserstein GAN\" paper](https://arxiv.org/abs/1701.07875) and the implementation in [TF-GAN](https://github.com/tensorflow/gan/blob/master/tensorflow_gan/python/losses/losses_impl.py), we'll just look at the sum/mean of the difference between values on generated and real images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wkMNfBWlT-PV"
      },
      "outputs": [],
      "source": [
        "#def discriminator_loss(real_output, fake_output):\n",
        "#    real_loss = cross_entropy(tf.ones_like(real_output), real_output)\n",
        "#    fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "#    total_loss = real_loss + fake_loss\n",
        "#    return total_loss\n",
        "def discriminator_loss(real_output, fake_output):\n",
        "    return tf.math.reduce_mean(fake_output) - tf.math.reduce_mean(real_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jd-3GCUEiKtv"
      },
      "source": [
        "### Generator loss\n",
        "The generator's loss quantifies how well it was able to trick the discriminator. Intuitively, if the generator is performing well, the discriminator will classify the fake images as real (output a positive value).\n",
        "Since this is a minimization problem, take the negative of the discriminator's output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "90BIcCKcDMxz"
      },
      "outputs": [],
      "source": [
        "#def generator_loss(fake_output):\n",
        "#    return cross_entropy(tf.ones_like(fake_output), fake_output)\n",
        "def generator_loss(fake_output):\n",
        "    return -tf.math.reduce_mean(fake_output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgIc7i0th_Iu"
      },
      "source": [
        "The discriminator and the generator optimizers are different since you will train two networks separately."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iWCn_PVdEJZ7"
      },
      "outputs": [],
      "source": [
        "steps_per_epoch = ds_length / BATCH_SIZE\n",
        "\n",
        "# A PRETTY GOOD BASELINE (32 batch size)\n",
        "#generator_optimizer = tf.keras.optimizers.Adam(5e-4, 0.5)\n",
        "#discriminator_optimizer = tf.keras.optimizers.Adam(1e-4, 0.5)\n",
        "\n",
        "# THIS SEEMS TO WORK WELL...\n",
        "base_gen_lr = 5e-4\n",
        "gen_lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    [60*steps_per_epoch, 60*steps_per_epoch, 60*steps_per_epoch], \n",
        "    [base_gen_lr,        base_gen_lr/2,     base_gen_lr/3,     base_gen_lr/4])\n",
        "generator_optimizer = tf.keras.optimizers.Adam(gen_lr_schedule, 0.5)\n",
        "base_dis_lr = 1e-4\n",
        "dis_lr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n",
        "    [60*steps_per_epoch, 60*steps_per_epoch, 60*steps_per_epoch], \n",
        "    [base_dis_lr,        base_dis_lr/2,     base_dis_lr/3,     base_dis_lr/4])\n",
        "discriminator_optimizer = tf.keras.optimizers.Adam(dis_lr_schedule, 0.5)\n",
        "\n",
        "# Slower progress but it kind of gets there\n",
        "#generator_optimizer = tf.keras.optimizers.Adam(3e-4, 0.5)\n",
        "#discriminator_optimizer = tf.keras.optimizers.Adam(3e-4, 0.9)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWtinsGDPJlV"
      },
      "source": [
        "### Save checkpoints\n",
        "This notebook also demonstrates how to save and restore models, which can be helpful in case a long running training task is interrupted."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CA1w-7s2POEy"
      },
      "outputs": [],
      "source": [
        "checkpoint_dir = \"./training_checkpoints\"\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(generator_optimizer=generator_optimizer,\n",
        "                                 discriminator_optimizer=discriminator_optimizer,\n",
        "                                 generator=generator,\n",
        "                                 discriminator=discriminator)\n",
        "\n",
        "# Checkpoint Manager could be helpful for deleting old checkpoints\n",
        "manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oG1p7L71aEcy"
      },
      "outputs": [],
      "source": [
        "# Restore from checkpoint, if possible\n",
        "if False:\n",
        "  latest_checkpoint = tf.train.latest_checkpoint(os.path.abspath(checkpoint_dir))\n",
        "  if latest_checkpoint is not None:\n",
        "    checkpoint.restore(latest_checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rw1fkAczTQYh"
      },
      "source": [
        "## Define the training loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NS2GWywBbAWo"
      },
      "outputs": [],
      "source": [
        "num_examples_to_generate = 16\n",
        "\n",
        "# You will reuse this seed over time\n",
        "# (to visualize progress in the animated GIF)\n",
        "seed = tf.random.normal([num_examples_to_generate, noise_dim])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jylSonrqSWfi"
      },
      "source": [
        "The training loop begins with generator receiving a random seed as input. That seed is used to produce an image. The discriminator is then used to classify real images (drawn from the training set) and fakes images (produced by the generator). The loss is calculated for each of these models, and the gradients are used to update the generator and discriminator."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3t5ibNo05jCB"
      },
      "outputs": [],
      "source": [
        "# Notice the use of `tf.function`\n",
        "# This annotation causes the function to be \"compiled\".\n",
        "@tf.function\n",
        "def train_step(images):\n",
        "    noise = tf.random.normal([BATCH_SIZE, noise_dim])\n",
        "\n",
        "    with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
        "      generated_images = generator(noise, training=True)\n",
        "\n",
        "      real_output = discriminator(images, training=True)\n",
        "      fake_output = discriminator(generated_images, training=True)\n",
        "\n",
        "      gen_loss = generator_loss(fake_output)\n",
        "      disc_loss = discriminator_loss(real_output, fake_output)\n",
        "\n",
        "    # Could probably combine these two steps into a call to `minimize()`\n",
        "    # TODO: consider multiple steps?\n",
        "    gradients_of_generator = gen_tape.gradient(gen_loss, generator.trainable_variables)\n",
        "    gradients_of_discriminator = disc_tape.gradient(disc_loss, discriminator.trainable_variables)\n",
        "\n",
        "    generator_optimizer.apply_gradients(zip(gradients_of_generator, generator.trainable_variables))\n",
        "    discriminator_optimizer.apply_gradients(zip(gradients_of_discriminator, discriminator.trainable_variables))\n",
        "\n",
        "    return gen_loss, disc_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2M7LmLtGEMQJ"
      },
      "outputs": [],
      "source": [
        "def train(dataset, epochs, g_losses=None, d_losses=None):\n",
        "\n",
        "  if g_losses is None: g_losses = []\n",
        "  if d_losses is None: d_losses = []\n",
        "\n",
        "  for epoch in range(epochs):\n",
        "    start = time.time()\n",
        "\n",
        "    for image_batch in dataset:\n",
        "      gloss, dloss = train_step(image_batch)\n",
        "      g_losses.append(gloss)\n",
        "      d_losses.append(dloss)\n",
        "\n",
        "    # Produce images for the GIF as you go\n",
        "    display.clear_output(wait=True)\n",
        "    generate_and_save_images(generator,\n",
        "                             epoch + 1,\n",
        "                             seed)\n",
        "\n",
        "    # Save the model every 15 epochs\n",
        "    if (epoch + 1) % 15 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print (\"Time for epoch {} is {} sec\".format(epoch + 1, time.time()-start))\n",
        "\n",
        "  # Generate after the final epoch\n",
        "  display.clear_output(wait=True)\n",
        "  generate_and_save_images(generator, epochs, seed)\n",
        "\n",
        "  return g_losses, d_losses"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2aFF7Hk3XdeW"
      },
      "source": [
        "**Generate and save images**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RmdVsmvhPxyy"
      },
      "outputs": [],
      "source": [
        "def generate_and_save_images(model, epoch, test_input):\n",
        "  # Notice `training` is set to False.\n",
        "  # This is so all layers run in inference mode (batchnorm).\n",
        "  predictions = model(test_input, training=False)\n",
        "\n",
        "  fig = plt.figure(figsize=(4, 4))\n",
        "\n",
        "  for i in range(predictions.shape[0]):\n",
        "      plt.subplot(4, 4, i+1)\n",
        "      plt.imshow(predictions[i, :, :, 0] * 127.5 + 127.5, cmap=\"gray\")\n",
        "      plt.axis('off')\n",
        "\n",
        "  #plt.savefig(\"image_at_epoch_{:04d}.png\".format(epoch))\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZvkqaAraIRY"
      },
      "outputs": [],
      "source": [
        "def plot_losses(g_losses, d_losses):\n",
        "  gl = np.asarray(g_losses)\n",
        "  dl = np.asarray(d_losses)\n",
        "  fig = plt.figure(figsize=(7, 4))\n",
        "  plt.plot(gl, 'b', label='gen. loss = -disc(fake)')\n",
        "  plt.plot(dl, 'r', label='disc. loss = disc(fake) - disc(real)')\n",
        "  plt.plot(gl + dl, 'k', label='sum = -disc(real)')\n",
        "  plt.legend()\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZrd4CdjR-Fp"
      },
      "source": [
        "## Train the model\n",
        "Call the `train()` method defined above to train the generator and discriminator simultaneously. Note, training GANs can be tricky. It's important that the generator and discriminator do not overpower each other (e.g., that they train at a similar rate)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPDJ85_D9q4B"
      },
      "outputs": [],
      "source": [
        "g_losses = []\n",
        "d_losses = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 496
        },
        "id": "Ly3UN0SLLY2l",
        "outputId": "2a90fd66-69b0-4c75-88d6-0a2063c49964"
      },
      "outputs": [],
      "source": [
        "EPOCHS = 120\n",
        "\n",
        "g_losses, d_losses = train(train_dataset, EPOCHS, g_losses, d_losses)\n",
        "plot_losses(g_losses, d_losses)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R16-ZSmC_Xnz"
      },
      "source": [
        "Save last checkpoint to Google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vzV9KgMA_VUa"
      },
      "outputs": [],
      "source": [
        "# files of last checkpoint:\n",
        "latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n",
        "checkpoint_files = glob.glob(latest_checkpoint+'*')\n",
        "checkpoint_files.append(checkpoint_dir+\"/checkpoint\")\n",
        "\n",
        "# copy to Drive\n",
        "target_dir = mount_point+\"/MyDrive/chaosGAN/ckpt_{}\".format(image_size)\n",
        "for f in checkpoint_files:\n",
        "  !cp $f $target_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MiqVkdXOI_T3"
      },
      "outputs": [],
      "source": [
        "# To unmount:\n",
        "# (make sure files have copied before unmounting)\n",
        "#drive.flush_and_unmount()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5x3q9_Oe5q0A"
      },
      "outputs": [],
      "source": [
        "# Display a single image using the epoch number\n",
        "#PIL.Image.open(\"image_at_epoch_{:04d}.png\".format(EPOCHS))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 248
        },
        "id": "rddfIqCrjdPS",
        "outputId": "f41dc24b-66f1-402a-a8ea-909f98093ce8"
      },
      "outputs": [],
      "source": [
        "noise = tf.random.normal([1, noise_dim])\n",
        "generated_image = generator(noise, training=False)\n",
        "plt.imshow(generated_image[0, :, :, 0], cmap='gray')\n",
        "plt.axis('off')\n",
        "plt.savefig(\"generated.png\")\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "GAN - TF2",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
